{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as et\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iter_docs(author, id, genero, variedad):\n",
    "    author_doc = {}\n",
    "    author_doc['id'] = id\n",
    "    author_doc['genero'] = genero\n",
    "    author_doc['variedad'] = variedad\n",
    "    \n",
    "    total = 0\n",
    "    tweet = ''\n",
    "    for doc in author.iter('document'):\n",
    "        doc_dict = author_doc.copy()\n",
    "        tweet = tweet +  \" \" + doc.text.replace(\"\\n\", \"\")\n",
    "        total += 1\n",
    "    \n",
    "    author_doc['total'] = total\n",
    "    author_doc['tweet'] = tweet\n",
    "    return author_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "truth = {}\n",
    "\n",
    "training = 'training'\n",
    "test = 'test'\n",
    "\n",
    "for dirname in [training, test]:\n",
    "    data[dirname] = pd.DataFrame()\n",
    "    truth[dirname] = pd.read_csv('./'+dirname+'/truth.txt', sep=\":::\", names = [\"id\", \"genero\", \"pais\"], header=None, engine='python')\n",
    "\n",
    "    for filename in os.listdir(dirname):\n",
    "        if filename == \"truth.txt\":\n",
    "            continue\n",
    "        f = './'+dirname+'/'+filename\n",
    "        parsed = et.parse(f)\n",
    "        \n",
    "        t = truth[dirname].loc[truth[dirname]['id'] == filename.replace(\".xml\", \"\")]\n",
    "        for index, row in t.iterrows():\n",
    "            genero = row[1]\n",
    "            variedad = row[2]\n",
    "\n",
    "        data[dirname] = data[dirname].append([iter_docs(parsed.getroot(), filename.replace(\".xml\", \"\"), genero, variedad)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genero</th>\n",
       "      <th>id</th>\n",
       "      <th>total</th>\n",
       "      <th>tweet</th>\n",
       "      <th>variedad</th>\n",
       "      <th>ctweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>f4c9c348fe3345a221b211a662e44f6b</td>\n",
       "      <td>100</td>\n",
       "      <td>ELN asesina a un policía y meten bomba en su ...</td>\n",
       "      <td>colombia</td>\n",
       "      <td>eln asesina a un policia y meten bomba en su c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>4c8b44a69798168e2d05d5e7d30f567c</td>\n",
       "      <td>100</td>\n",
       "      <td>NO DUERMAN QUE SE VA!!!! https://t.co/QxP5vjF...</td>\n",
       "      <td>argentina</td>\n",
       "      <td>no duerman que se va ! ! ! https://t.co/QxP5vj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>8bdf815f5b76cfebc25ae51d5f5ab054</td>\n",
       "      <td>100</td>\n",
       "      <td>@santorendon por lo que cuenta es como primo ...</td>\n",
       "      <td>colombia</td>\n",
       "      <td>por lo que cuenta es como primo lejano de #har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>35ccb63279d819182acbe99ace6106c6</td>\n",
       "      <td>100</td>\n",
       "      <td>raspando la olla https://t.co/DpdOOroknp si n...</td>\n",
       "      <td>venezuela</td>\n",
       "      <td>raspando la olla https://t.co/DpdOOroknp si no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>3b8bff5ba21d4e76c2e239b8e4a30c1d</td>\n",
       "      <td>100</td>\n",
       "      <td>Así mismo es... https://t.co/ugqXINZHYi Así e...</td>\n",
       "      <td>mexico</td>\n",
       "      <td>asi mismo es ... https://t.co/ugqXINZHYi asi e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   genero                                id  total  \\\n",
       "0  female  f4c9c348fe3345a221b211a662e44f6b    100   \n",
       "0    male  4c8b44a69798168e2d05d5e7d30f567c    100   \n",
       "0    male  8bdf815f5b76cfebc25ae51d5f5ab054    100   \n",
       "0    male  35ccb63279d819182acbe99ace6106c6    100   \n",
       "0    male  3b8bff5ba21d4e76c2e239b8e4a30c1d    100   \n",
       "\n",
       "                                               tweet   variedad  \\\n",
       "0   ELN asesina a un policía y meten bomba en su ...   colombia   \n",
       "0   NO DUERMAN QUE SE VA!!!! https://t.co/QxP5vjF...  argentina   \n",
       "0   @santorendon por lo que cuenta es como primo ...   colombia   \n",
       "0   raspando la olla https://t.co/DpdOOroknp si n...  venezuela   \n",
       "0   Así mismo es... https://t.co/ugqXINZHYi Así e...     mexico   \n",
       "\n",
       "                                              ctweet  \n",
       "0  eln asesina a un policia y meten bomba en su c...  \n",
       "0  no duerman que se va ! ! ! https://t.co/QxP5vj...  \n",
       "0  por lo que cuenta es como primo lejano de #har...  \n",
       "0  raspando la olla https://t.co/DpdOOroknp si no...  \n",
       "0  asi mismo es ... https://t.co/ugqXINZHYi asi e...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n",
    "\n",
    "def clean(text):\n",
    "    tweet = text.replace(\"\\\\n\", \"\")\n",
    "    #tokenizamos el tweet\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    #quitmos acentos\n",
    "    return unidecode.unidecode(\" \".join(tokens))\n",
    "\n",
    "for dirname in [training, test]:\n",
    "    data[dirname]['ctweet'] = data[dirname].apply(lambda row: clean(row['tweet']), axis=1)\n",
    "\n",
    "data[training].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nuevas características\n",
    "emoticons_pattern = re.compile(\"[\\U00010000-\\U0010ffff]\", flags=re.UNICODE)\n",
    "def emoticons(text):\n",
    "    count = 0\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    for t in tokens:\n",
    "        search = emoticons_pattern.findall(t)\n",
    "        count += len(search)\n",
    "    return count\n",
    "\n",
    "mentions_pattern = re.compile(\"@\\w+\")\n",
    "def mentions(text):\n",
    "    users = mentions_pattern.findall(text)\n",
    "    return len(users)\n",
    "\n",
    "hashtags_pattern = re.compile(\"#\\w+\")\n",
    "def hashtags(text):\n",
    "    hashes = hashtags_pattern.findall(text)\n",
    "    return len(hashes)\n",
    "\n",
    "url_pattern = re.compile(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "def readurl(text):\n",
    "    urls = url_pattern.findall(text)\n",
    "    return len(urls)\n",
    "\n",
    "def tokenscount(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "retweets_pattern = re.compile(\"(RT|via)((?:\\b\\W*@\\w+)+)\")\n",
    "def retweets(text):\n",
    "    rts = retweets_pattern.findall(text)\n",
    "    return len(rts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformaciones = [\n",
    "    lambda x: len(x),\n",
    "    lambda x: x.count(\" \"),\n",
    "    lambda x: x.count(\".\"),\n",
    "    lambda x: x.count(\"!\"),\n",
    "    lambda x: x.count(\"?\"),\n",
    "    lambda x: len(x) / (x.count(\" \") + 1),\n",
    "    lambda x: x.count(\" \") / (x.count(\".\") + 1),\n",
    "    lambda x: len(re.findall(\"\\d\", x)),\n",
    "    lambda x: len(re.findall(\"[A-Z]\", x)),\n",
    "    lambda x: emoticons(x),\n",
    "    lambda x: mentions(x),\n",
    "    lambda x: hashtags(x),\n",
    "    lambda x: readurl(x),\n",
    "    lambda x: tokenscount(x),\n",
    "    lambda x: retweets(x)\n",
    "]\n",
    "\n",
    "meta = {}\n",
    "for d in [training, test]:\n",
    "    meta[d] = []\n",
    "    for func in transformaciones:\n",
    "        meta[d].append(data[d]['tweet'].apply(func)/data[d]['total'])\n",
    "        \n",
    "meta[training] = np.asarray(meta[training]).T\n",
    "meta[test] = np.asarray(meta[test]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.18540000e+02,   1.61400000e+01,   6.40000000e-01,\n",
       "          5.20000000e-01,   7.00000000e-02,   7.33993808e-02,\n",
       "          2.48307692e-01,   6.90000000e-01,   7.44000000e+00,\n",
       "          5.00000000e-02,   1.12000000e+00,   8.80000000e-01,\n",
       "          3.40000000e-01,   1.65400000e+01,   0.00000000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[training][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import time\n",
    "from sklearn import decomposition\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printModels():\n",
    "    for key in models:\n",
    "        start = time.time()\n",
    "        model = models[key].fit(features, y_train.ravel())\n",
    "        predicted = model.predict(features_test)\n",
    "        end = time.time()\n",
    "        print(key, accuracy_score(y_test,predicted), \"Time:\", end-start)\n",
    "        \n",
    "        \n",
    "stemmer = PorterStemmer()\n",
    "trans_table = {ord(c): None for c in string.punctuation + string.digits} \n",
    "def tokenize(text):\n",
    "    tokens = [word for word in nltk.word_tokenize(text.translate(trans_table)) if len(word) > 1] #if len(word) > 1 because I only want to retain words that are at least two characters before stemming, although I can't think of any such words that are not also stopwords\n",
    "    stems = [stemmer.stem(item) for item in tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 118.54   16.14    0.64 ...,    0.      0.      0.  ]]\n"
     ]
    }
   ],
   "source": [
    "#genero\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words=stopwords.words('spanish'),\\\n",
    "                            ngram_range=(1,2), min_df=10, max_features=8000,norm='l2',\\\n",
    "                            use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "  \n",
    "y_train=data[training]['genero'].values\n",
    "X_train = vectorizer.fit_transform(data[training]['ctweet'])\n",
    "\n",
    "features = np.hstack([meta[training], X_train.todense()])\n",
    "                      \n",
    "#creando bolsa de palabras por género\n",
    "#data['male'] = data[training][data[training].genero=='male']\n",
    "#data['female'] = data[training][data[training].genero=='female']\n",
    "\n",
    "#vectorizer_male = TfidfVectorizer(analyzer=\"word\", max_features=200, tokenizer=tokenize)\n",
    "#vectorizer_male.fit(data['male']['ctweet'])\n",
    "#X_train_male = vectorizer_male.transform(data[training]['ctweet'])\n",
    "\n",
    "#vectorizer_female = TfidfVectorizer(analyzer=\"word\", max_features=200, tokenizer=tokenize)\n",
    "#vectorizer_female.fit(data['female']['ctweet'])\n",
    "#X_train_female = vectorizer_female.transform(data[training]['ctweet'])\n",
    "\n",
    "#features = np.hstack([meta[training], X_train.todense(), X_train_male.todense(), X_train_female.todense()])\n",
    "\n",
    "print(features[:1])\n",
    "\n",
    "y_test=data[test]['genero'].values\n",
    "X_test=vectorizer.transform(data[test]['ctweet'])\n",
    "\n",
    "#creando bolsa de palabras por género\n",
    "#X_test_male = vectorizer_male.transform(data[test]['ctweet'])\n",
    "#X_test_female = vectorizer_female.transform(data[test]['ctweet'])\n",
    "\n",
    "#features_test = np.hstack([meta[test], X_test.todense(), X_test_male.todense(), X_test_female.todense()])\n",
    "features_test = np.hstack([meta[test], X_test.todense()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L 0.592142857143 Time: 3.0182721614837646\n",
      "NB 0.686428571429 Time: 0.1158747673034668\n",
      "R 0.645714285714 Time: 0.7247958183288574\n",
      "KN 0.577142857143 Time: 6.471596002578735\n",
      "LO 0.769285714286 Time: 0.6107280254364014\n",
      "NE 0.775714285714 Time: 75.25347208976746\n"
     ]
    }
   ],
   "source": [
    "models={\n",
    "    \"L\": LinearSVC(),\n",
    "    \"NB\": MultinomialNB(),\n",
    "    \"R\": RandomForestClassifier(),\n",
    "    \"KN\": KNeighborsClassifier(),\n",
    "    \"LO\": LogisticRegression(),\n",
    "    \"NE\": MLPClassifier()\n",
    "}\n",
    "\n",
    "    \n",
    "printModels()\n",
    "\n",
    "#con bolsa de palabras por género\n",
    "#L 0.681428571429 Time: 4.463599920272827\n",
    "#NB 0.677857142857 Time: 0.08524703979492188\n",
    "#R 0.673571428571 Time: 0.7116379737854004\n",
    "#KN 0.587142857143 Time: 6.8814311027526855\n",
    "#LO 0.775 Time: 0.902728796005249\n",
    "#NE 0.774285714286 Time: 69.29499626159668"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#variedad del castellano\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words=stopwords.words('spanish'),\\\n",
    "                            min_df=50, max_features=8000,norm='l2',\\\n",
    "                            use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "  \n",
    "y_train=data[training]['variedad'].values\n",
    "X_train = vectorizer.fit_transform(data[training]['ctweet'])\n",
    "\n",
    "y_test=data[test]['variedad'].values\n",
    "X_test=vectorizer.transform(data[test]['ctweet'])\n",
    "\n",
    "features = np.hstack([meta[training], X_train.todense()])\n",
    "features_test = np.hstack([meta[test], X_test.todense()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L 0.676428571429 Time: 14.360452890396118\n",
      "NB 0.732142857143 Time: 0.04561209678649902\n",
      "R 0.831428571429 Time: 0.46947312355041504\n",
      "KN 0.23 Time: 3.02897310256958\n",
      "LO 0.936428571429 Time: 2.371469020843506\n",
      "NE 0.937857142857 Time: 23.478174924850464\n"
     ]
    }
   ],
   "source": [
    "models={\n",
    "    \"L\": LinearSVC(),\n",
    "    \"NB\": MultinomialNB(),\n",
    "    \"R\": RandomForestClassifier(),\n",
    "    \"KN\": KNeighborsClassifier(),\n",
    "    \"LO\": LogisticRegression(),\n",
    "    \"NE\": MLPClassifier()\n",
    "}\n",
    "\n",
    "\n",
    "printModels()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
